{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paragraph & Token Delimiters\n",
    "para_pat = r'\\n+'\n",
    "token_pat = r'([\\W_]+)'\n",
    "db_file = 'ted-talks.db' # Output sqlite filename\n",
    "\n",
    "# OCHO properties\n",
    "OHCO = ['speaker', 'event', 'id', 'para_num', 'sent_num', 'token_num']\n",
    "AUTHS = OHCO[:1]\n",
    "BOOKS = OHCO[:2]\n",
    "CHAPS = OHCO[:3]\n",
    "PARAS = OHCO[:4]\n",
    "SENTS = OHCO[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><a href=\"www.theodorespeaks.com\">Theodore Speaks Dataset</a></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ot_df = pd.read_csv(\"ted-talks/TED_Talks_by_ID_plus-transcripts-and-LIWC-and-MFT-plus-views.csv\")\n",
    "ot_df.columns = np.char.lower(ot_df.columns.values.astype(str))\n",
    "ot_df[\"date_published\"] = pd.to_datetime(ot_df[\"date_published\"])\n",
    "ot_df[\"views\"] = ot_df.pop(\"views_as_of_06162017\")\n",
    "ot_df[\"duration\"] = pd.to_timedelta(ot_df[\"duration\"], unit='s').dt.seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Scraped Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape = pd.read_csv(\"ted-talks/ted-talks-scraped.csv\").set_index(\"id\")\n",
    "scrape_liwc = pd.read_csv(\"ted-talks/ted-talks-scraped-LIWC.csv\")\n",
    "scrape_liwc[\"id\"] = scrape_liwc.pop(\"Source (F)\")\n",
    "\n",
    "print(list(scrape.columns.values))\n",
    "\n",
    "liwc_col = list(scrape_liwc.columns.values)\n",
    "liwc_col = liwc_col[liwc_col.index(\"WC\"):]\n",
    "scrape_liwc = scrape_liwc[liwc_col]\n",
    "\n",
    "scrape_df = scrape.join(scrape_liwc.set_index('id')).reset_index()\n",
    "\n",
    "scrape_df[\"speaker\"] = scrape_df.pop(\"main_speaker\")\n",
    "scrape_df[\"date_published\"] = pd.to_datetime(pd.to_datetime(scrape_df.pop(\"published_date\"), unit='s').dt.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted_talks = pd.merge(scrape_df, ot_df, how=\"outer\", on=list(ot_df.columns.intersection(scrape_df.columns)))\n",
    "ted_talks = ted_talks.sort_values('id').drop_duplicates(subset='id', keep='first')\n",
    "ted_talks[\"events\"] = ted_talks[\"transcript\"].str.extractall(r'(\\([^)]*\\))').unstack().apply(lambda x:','.join(x.dropna()), axis=1).str.replace(r\"\\(|\\)\",\"\").str.split(\",\")\n",
    "ted_talks[\"transcript\"] = ted_talks[\"transcript\"].str.replace(r'\\([^)]*\\)|([01][0-9]):[0-5][0-9]|([0-9]):[0-5][0-9]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ted_talks[scrape_df.columns.values].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('interactive-visual/ted-talks.json', 'w') as f:\n",
    "    f.write(ted_talks[scrape_df.columns.values].to_json(orient='records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Create F4 compliant version of corpus</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = ted_talks[[\"speaker\",\"event\",\"id\",\"transcript\"]]\n",
    "\n",
    "try:\n",
    "    T = T.set_index(CHAPS)\n",
    "    T = T.sort_index()\n",
    "except KeyError:\n",
    "    pass\n",
    "\n",
    "T = T[T[\"transcript\"].str.strip() != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paras = T.transcript.str.split(para_pat, expand=True).stack().to_frame().rename(columns={0:'para_str'})\n",
    "paras.index.names = PARAS\n",
    "paras.para_str = paras.para_str.str.strip()\n",
    "paras.para_str = paras.para_str.str.replace(r'\\n', ' ')\n",
    "paras.para_str = paras.para_str.str.replace(r'\\s+', ' ')\n",
    "paras = paras[~paras.para_str.str.match(r'^\\s*$')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = paras.para_str.apply(lambda x: pd.Series(nltk.sent_tokenize(x))).stack().to_frame().rename(columns={0:'sent_str'})\n",
    "sents.index.names = SENTS\n",
    "del(paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('\\s+', gaps=True)\n",
    "tokens = sents.sent_str.apply(lambda x: pd.Series(nltk.pos_tag(tokenizer.tokenize(x)))).stack().to_frame().rename(columns={0:'pos_tuple'})\n",
    "tokens.index.names = OHCO\n",
    "tokens['pos'] = tokens.pos_tuple.apply(lambda x: x[1])\n",
    "tokens['token_str'] = tokens.pos_tuple.apply(lambda x: x[0])\n",
    "tokens = tokens.drop('pos_tuple', 1)\n",
    "tokens['punc'] = tokens.token_str.str.match(r'^[\\W_]*$').astype('int')\n",
    "tokens['num'] = tokens.token_str.str.match(r'^.*\\d.*$').astype('int')\n",
    "tokens.loc[(tokens.punc == 0) & (tokens.num == 0), 'term_str'] = tokens.token_str.str.lower().str.replace(token_pat, '')\n",
    "del(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokens[tokens.punc == 0].term_str.value_counts().to_frame().reset_index().rename(columns={'index':'term_str', 'term_str':'n'})\n",
    "vocab = vocab.sort_values('term_str').reset_index(drop=True)\n",
    "vocab['p'] = vocab.n / vocab.n.sum()\n",
    "vocab.index.name = 'term_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "vocab['port_stem'] = vocab.term_str.apply(lambda x: stemmer.stem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = pd.DataFrame({'x':1}, index=set(nltk.corpus.stopwords.words('english')))\n",
    "vocab['stop'] = vocab.term_str.map(sw.x).fillna(0).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens['term_id'] = tokens['term_str'].map(vocab.reset_index().set_index('term_str').term_id).fillna(-1).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.head()\n",
    "vocab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(db_file) as db:\n",
    "    T.to_sql('doc', db, if_exists='replace', index=True)\n",
    "    tokens.to_sql('token', db, if_exists='replace', index=True)\n",
    "    vocab.to_sql('vocab', db, if_exists='replace', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
